{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699c15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re, glob\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "top = os.getcwd()\n",
    "data_dir = os.path.join(top, 'data') # Specify data directoy here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab578b",
   "metadata": {},
   "source": [
    "# Parse pdf File into csv\n",
    "\n",
    "- extract data from pdf file into individual csv file in each folder run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcbecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table(text):\n",
    "    \"\"\"\n",
    "    Parse table from GC report pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: string\n",
    "        string from reading report\n",
    "    \"\"\"\n",
    "\n",
    "    if \"No peaks found\" in text: \n",
    "        return pd.DataFrame(columns=[\"#\", \"RetTime [min]\", \"Type\", \"Width [min]\", \"Area [pA*s]\", \"Height [pA]\", \"Area %\"])\n",
    "\n",
    "    pattern = r\"----\\|-------\\|----\\|-------\\|----------\\|----------\\|--------\\|\\n(.*?)Totals\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if not match:\n",
    "        return pd.DataFrame(columns=[\"#\", \"RetTime [min]\", \"Type\", \"Width [min]\", \"Area [pA*s]\", \"Height [pA]\", \"Area %\"])\n",
    "\n",
    "    table = match.group(1).strip() \n",
    "\n",
    "    lines = [line.strip() for line in table.strip().splitlines() if line.strip()] \n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        parts = re.split(r\"\\s+\", line)\n",
    "        if len(parts) == 7:\n",
    "            data.append({\n",
    "                \"#\": int(parts[0]),\n",
    "                \"RetTime [min]\": float(parts[1]),\n",
    "                \"Type\": parts[2],\n",
    "                \"Width [min]\": float(parts[3]),\n",
    "                \"Area [pA*s]\": float(parts[4].replace(\"e\", \"E\")),\n",
    "                \"Height [pA]\": float(parts[5]),\n",
    "                \"Area %\": float(parts[6])\n",
    "            })\n",
    "        elif len(parts) == 8:\n",
    "            data.append({\n",
    "                \"#\": int(parts[0]),\n",
    "                \"RetTime [min]\": float(parts[1]),\n",
    "                \"Type\": parts[2] + parts[3],\n",
    "                \"Width [min]\": float(parts[4]),\n",
    "                \"Area [pA*s]\": float(parts[5].replace(\"e\", \"E\")),\n",
    "                \"Height [pA]\": float(parts[6]),\n",
    "                \"Area %\": float(parts[7])\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8dc2a5",
   "metadata": {},
   "source": [
    "- pdf_unmatch lists all data pdf files in the wrong data folder or wrong file name\n",
    "- seq_unmatch lists all data pdf files that has a wrong sequence (sanity check)\n",
    "- the number ouputted by the cell below shows how many files failed to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea00ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pdf_unmatch = []\n",
    "seq_unmatch = []\n",
    "errs = []\n",
    "\n",
    "for path in glob.glob(os.path.join(data_dir, '*')):\n",
    "    if os.path.basename(path)[:2] == 'F-':\n",
    "        try:\n",
    "            sequence = int(os.path.basename(path)[2:5])\n",
    "            name = os.path.basename(path)[9:]\n",
    "            for pdf in glob.glob(os.path.join(path, '*.PDF')):\n",
    "                reader = PdfReader(pdf)\n",
    "                possible_pages = [1, 2]   \n",
    "\n",
    "                text = \"\"\n",
    "                for p in possible_pages:\n",
    "                    if p < len(reader.pages):\n",
    "                        t = reader.pages[p].extract_text() or \"\"\n",
    "                        if \"Seq. Line\" in t: \n",
    "                            text = t\n",
    "                            break\n",
    "\n",
    "                # text = reader.pages[1, 2].extract_text() # might need to change page numeber if the data is on a different page\n",
    "                match = re.search(r\"Seq\\. Line\\s*:\\s*(\\d+)\", text)\n",
    "                if not match:\n",
    "                    pdf_unmatch.append(pdf) \n",
    "                elif not sequence == int(match.group(1)):\n",
    "                    seq_unmatch.append(pdf) \n",
    "                \n",
    "                \n",
    "                df = parse_table(text)\n",
    "                df.set_index(\"#\", inplace=True)\n",
    "                data = os.path.join(path, 'data.csv')\n",
    "                df.to_csv(data)\n",
    "\n",
    "        except Exception as e:\n",
    "            errs.append((sequence, e))\n",
    "\n",
    "print(pdf_unmatch)\n",
    "print(seq_unmatch)\n",
    "print(len(errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af40a3c",
   "metadata": {},
   "source": [
    "# Sample Peak Area Retention Time Collection\n",
    "\n",
    "- create a complete peak area dataframe for each solution\n",
    "    - each row is a sample injection\n",
    "    - two columns for each unique peak retention times: \n",
    "        1. peak area for common retention time (3 decimal truncation) \n",
    "        2. actual retention time value\n",
    "    - peaks with retention times 0.1 apart are merged\n",
    "\n",
    "## Collect Unique RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c57613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "[5.5, 5.6, 5.9, 6.3, 6.4, 6.5, 7.6, 7.7, 8.1, 8.2, 8.8, 8.9, 11.1, 11.2, 13.7, 13.8, 14.1, 16.0, 16.1, 18.7, 19.5, 19.7, 19.8]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>5.5_Peak_Area</th>\n",
       "      <th>5.5_actual-RT</th>\n",
       "      <th>5.6_Peak_Area</th>\n",
       "      <th>5.6_actual-RT</th>\n",
       "      <th>5.9_Peak_Area</th>\n",
       "      <th>5.9_actual-RT</th>\n",
       "      <th>6.3_Peak_Area</th>\n",
       "      <th>6.3_actual-RT</th>\n",
       "      <th>6.4_Peak_Area</th>\n",
       "      <th>...</th>\n",
       "      <th>16.1_Peak_Area</th>\n",
       "      <th>16.1_actual-RT</th>\n",
       "      <th>18.7_Peak_Area</th>\n",
       "      <th>18.7_actual-RT</th>\n",
       "      <th>19.5_Peak_Area</th>\n",
       "      <th>19.5_actual-RT</th>\n",
       "      <th>19.7_Peak_Area</th>\n",
       "      <th>19.7_actual-RT</th>\n",
       "      <th>19.8_Peak_Area</th>\n",
       "      <th>19.8_actual-RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, 5.5_Peak_Area, 5.5_actual-RT, 5.6_Peak_Area, 5.6_actual-RT, 5.9_Peak_Area, 5.9_actual-RT, 6.3_Peak_Area, 6.3_actual-RT, 6.4_Peak_Area, 6.4_actual-RT, 6.5_Peak_Area, 6.5_actual-RT, 7.6_Peak_Area, 7.6_actual-RT, 7.7_Peak_Area, 7.7_actual-RT, 8.1_Peak_Area, 8.1_actual-RT, 8.2_Peak_Area, 8.2_actual-RT, 8.8_Peak_Area, 8.8_actual-RT, 8.9_Peak_Area, 8.9_actual-RT, 11.1_Peak_Area, 11.1_actual-RT, 11.2_Peak_Area, 11.2_actual-RT, 13.7_Peak_Area, 13.7_actual-RT, 13.8_Peak_Area, 13.8_actual-RT, 14.1_Peak_Area, 14.1_actual-RT, 16.0_Peak_Area, 16.0_actual-RT, 16.1_Peak_Area, 16.1_actual-RT, 18.7_Peak_Area, 18.7_actual-RT, 19.5_Peak_Area, 19.5_actual-RT, 19.7_Peak_Area, 19.7_actual-RT, 19.8_Peak_Area, 19.8_actual-RT]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_RT = set()\n",
    "\n",
    "for path in glob.glob(os.path.join(data_dir, '*')):\n",
    "    if os.path.basename(path)[:2] == 'F-':\n",
    "        id = os.path.basename(path)\n",
    "\n",
    "        data = os.path.join(path, 'data.csv')\n",
    "        # data = os.path.join(path, 'REPORT01.CSV')\n",
    "        df_iter = pd.read_csv(data)\n",
    "        \n",
    "        for index, row in df_iter.iterrows():\n",
    "            RT_trunc = int(row['RetTime [min]'] * 10) / 10\n",
    "            unique_RT.add(RT_trunc)\n",
    "            # print(f\"retention time: {row['RetTime [min]']}; peak area: {row['Area [pA*s]']}\")\n",
    "        \n",
    "unique_RT = sorted(unique_RT)\n",
    "print(len(unique_RT))\n",
    "print(unique_RT)\n",
    "\n",
    "columns = []\n",
    "for rt in unique_RT:\n",
    "    columns.append(f\"{rt}_Peak_Area\")\n",
    "    columns.append(f\"{rt}_actual-RT\")\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df.insert(0, 'name', None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b773de3",
   "metadata": {},
   "source": [
    "# Merge Retention Times that are 0.1 apart\n",
    "- These are the same peak but artificially divided by truncating the decimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262d695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df):\n",
    "    \"\"\"\n",
    "    Merge Columns with Retention Time difference of 0.1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        dataframe of peak area anc actual retention times \n",
    "    \"\"\"\n",
    "    pattern = r'\\d+\\.\\d+'\n",
    "    RTs = set()\n",
    "    for col in df.columns:\n",
    "        match = re.findall(pattern, col)\n",
    "        for m in match:\n",
    "            RTs.add(float(m))\n",
    "\n",
    "    RTs = sorted(RTs)\n",
    "    pairs = []\n",
    "    triples = []\n",
    "\n",
    "    for i in range(len(RTs) - 1): # detect pairs\n",
    "        if round(RTs[i+1] - RTs[i], 1) == 0.1:\n",
    "            pairs.append([RTs[i], RTs[i+1]])\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    while i < len(pairs) - 1: # detect triples\n",
    "        a1, a2 = pairs[i]\n",
    "        b1, b2 = pairs[i+1]\n",
    "\n",
    "        if a2 == b1:\n",
    "            triples.append([a1, a2, b2])\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    pairs_used = set()\n",
    "    for t in triples:\n",
    "        pairs_used.add((t[0], t[1]))\n",
    "        pairs_used.add((t[1], t[2]))\n",
    "\n",
    "    pairs = [p for p in pairs if tuple(p) not in pairs_used]\n",
    "    \n",
    "    print(pairs)\n",
    "    print(triples)\n",
    "\n",
    "    if len(pairs) > 0: # collapse pairs\n",
    "        for pair in pairs:\n",
    "            col0_lower = f\"{pair[0]}_Peak_Area\"\n",
    "            col0_higher = f\"{pair[1]}_Peak_Area\"\n",
    "            col1_lower = f\"{pair[0]}_actual-RT\"\n",
    "            col1_higher = f\"{pair[1]}_actual-RT\"\n",
    "\n",
    "            low_n = df[col0_lower].count()\n",
    "            high_n = df[col0_higher].count()\n",
    "\n",
    "            if low_n > high_n:\n",
    "                for index, row in df.iterrows():\n",
    "                    if pd.isna(df.loc[index, col0_lower]):\n",
    "                        df.loc[index, col0_lower] = df.loc[index, col0_higher]\n",
    "                        df.loc[index, col1_lower] = df.loc[index, col1_higher]\n",
    "                df = df.drop(columns=[col0_higher, col1_higher])\n",
    "            else:\n",
    "                for index, row in df.iterrows():\n",
    "                    if pd.isna(df.loc[index, col0_higher]):\n",
    "                        df.loc[index, col0_higher] = df.loc[index, col0_lower]\n",
    "                        df.loc[index, col1_higher] = df.loc[index, col1_lower]\n",
    "                df = df.drop(columns=[col0_lower, col1_lower])\n",
    "\n",
    "    if len(triples) > 0: # collapse triples\n",
    "        for triple in triples:\n",
    "            low, mid, high = triple\n",
    "\n",
    "            col0 = {\n",
    "                \"low\": f\"{low}_Peak_Area\",\n",
    "                \"mid\": f\"{mid}_Peak_Area\",\n",
    "                \"high\": f\"{high}_Peak_Area\"\n",
    "            }\n",
    "            col1 = {\n",
    "                \"low\": f\"{low}_actual-RT\",\n",
    "                \"mid\": f\"{mid}_actual-RT\",\n",
    "                \"high\": f\"{high}_actual-RT\"\n",
    "            }\n",
    "            counts = {\n",
    "                \"low\": df[col0[\"low\"]].count(),\n",
    "                \"mid\": df[col0[\"mid\"]].count(),\n",
    "                \"high\": df[col0[\"high\"]].count()\n",
    "            }\n",
    "\n",
    "            primary = \"mid\"\n",
    "            secondaries = [\"low\", \"high\"]\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                if pd.isna(df.loc[idx, col0[primary]]):\n",
    "                    if not pd.isna(df.loc[idx, col0[secondaries[0]]]):\n",
    "                        df.loc[idx, col0[primary]] = df.loc[idx, col0[secondaries[0]]]\n",
    "                        df.loc[idx, col1[primary]] = df.loc[idx, col1[secondaries[0]]]\n",
    "                    elif not pd.isna(df.loc[idx, col0[secondaries[1]]]):\n",
    "                        df.loc[idx, col0[primary]] = df.loc[idx, col0[secondaries[1]]]\n",
    "                        df.loc[idx, col1[primary]] = df.loc[idx, col1[secondaries[1]]]\n",
    "            df = df.drop(\n",
    "                columns=[\n",
    "                    col0[secondaries[0]], col1[secondaries[0]],\n",
    "                    col0[secondaries[1]], col1[secondaries[1]]\n",
    "                ]\n",
    "            )\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a911a2",
   "metadata": {},
   "source": [
    "## Collect all sample into total.csv in peak-area/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f31467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.5, 5.6], [7.6, 7.7], [8.1, 8.2], [8.8, 8.9], [11.1, 11.2], [13.7, 13.8], [16.0, 16.1], [19.7, 19.8]]\n",
      "[[6.3, 6.4, 6.5]]\n"
     ]
    }
   ],
   "source": [
    "for path in glob.glob(os.path.join(data_dir, '*')):\n",
    "    if os.path.basename(path)[:2] == 'F-':\n",
    "        id = os.path.basename(path)\n",
    "        name = os.path.basename(path)[9:]\n",
    "\n",
    "        data = os.path.join(path, 'data.csv')\n",
    "        df_iter = pd.read_csv(data)\n",
    "        \n",
    "        row_insert = {}\n",
    "        for index, row in df_iter.iterrows():\n",
    "            RT_trunc = int(row['RetTime [min]'] * 10) / 10\n",
    "            row_insert[f\"{RT_trunc}_Peak_Area\"] = row['Area [pA*s]']\n",
    "            row_insert[f\"{RT_trunc}_actual-RT\"] = row['RetTime [min]']\n",
    "            row_insert['name'] = name\n",
    "        \n",
    "        df.loc[id] = row_insert\n",
    "\n",
    "peakdir = os.path.join(top, 'peak-area')\n",
    "if os.path.exists(peakdir) == False:\n",
    "    os.mkdir(peakdir)\n",
    "df = merge(df)\n",
    "df.to_csv(os.path.join(peakdir, 'total.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5235ac0",
   "metadata": {},
   "source": [
    "## Isolate each Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0626b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CL_1.D', 'CL_2.D', 'CL_3.D', 'CL_4.D', 'CL_whiskey_08.D', 'CL_ROH_07.D', 'CL_Lismore.D', 'CL_Clarlyle.D', 'CL_Evan.D'] # change solution names to whatever\n",
    "\n",
    "for name in names:\n",
    "    df_name = df[df['name'] == name]\n",
    "    df_name = df_name.dropna(axis=1, how='all')\n",
    "    base, ext = os.path.splitext(name)\n",
    "    df_name.to_csv(os.path.join(peakdir, f'{base}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009887a9",
   "metadata": {},
   "source": [
    "# Compute Mean and Standard Deviations\n",
    "\n",
    "- Create a new DataFrame with mean and standard deviations and confidence intervals\n",
    "    - Rows are samples\n",
    "    - Columns are average peak area, average standard deviations and average retention times for each RTs\n",
    "- saved in peak_area.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34720f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.6, 5.9, 6.4, 7.6, 8.1, 8.9, 11.1, 13.8, 14.1, 16.0, 18.7, 19.5, 19.7]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_samples</th>\n",
       "      <th>5.6_n_samples</th>\n",
       "      <th>5.6_avg_Area</th>\n",
       "      <th>5.6_std_Area</th>\n",
       "      <th>5.6_avg_RT</th>\n",
       "      <th>5.9_n_samples</th>\n",
       "      <th>5.9_avg_Area</th>\n",
       "      <th>5.9_std_Area</th>\n",
       "      <th>5.9_avg_RT</th>\n",
       "      <th>6.4_n_samples</th>\n",
       "      <th>...</th>\n",
       "      <th>18.7_std_Area</th>\n",
       "      <th>18.7_avg_RT</th>\n",
       "      <th>19.5_n_samples</th>\n",
       "      <th>19.5_avg_Area</th>\n",
       "      <th>19.5_std_Area</th>\n",
       "      <th>19.5_avg_RT</th>\n",
       "      <th>19.7_n_samples</th>\n",
       "      <th>19.7_avg_Area</th>\n",
       "      <th>19.7_std_Area</th>\n",
       "      <th>19.7_avg_RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CL_1</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.722588</td>\n",
       "      <td>0.436106</td>\n",
       "      <td>5.6264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>257.989241</td>\n",
       "      <td>7.098318</td>\n",
       "      <td>19.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_2</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.468092</td>\n",
       "      <td>0.189681</td>\n",
       "      <td>5.6106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>526.351981</td>\n",
       "      <td>8.239846</td>\n",
       "      <td>19.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_3</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.980861</td>\n",
       "      <td>0.198321</td>\n",
       "      <td>5.6106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>4.900047</td>\n",
       "      <td>0.181932</td>\n",
       "      <td>19.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_4</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.732605</td>\n",
       "      <td>0.211640</td>\n",
       "      <td>5.6058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>60.634962</td>\n",
       "      <td>1.624316</td>\n",
       "      <td>19.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_whiskey_08</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>7.834658</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>19.785111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_ROH_07</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.785</td>\n",
       "      <td>9.0</td>\n",
       "      <td>34.402772</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>19.573222</td>\n",
       "      <td>9</td>\n",
       "      <td>14.742714</td>\n",
       "      <td>0.234583</td>\n",
       "      <td>19.783667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_Lismore</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.84904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.937</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37.887647</td>\n",
       "      <td>0.264937</td>\n",
       "      <td>19.576000</td>\n",
       "      <td>9</td>\n",
       "      <td>18.654211</td>\n",
       "      <td>0.283424</td>\n",
       "      <td>19.785222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_Clarlyle</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.188203</td>\n",
       "      <td>0.063288</td>\n",
       "      <td>19.583222</td>\n",
       "      <td>9</td>\n",
       "      <td>5.867513</td>\n",
       "      <td>0.161361</td>\n",
       "      <td>19.793111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL_Evan</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.507867</td>\n",
       "      <td>0.030428</td>\n",
       "      <td>19.561333</td>\n",
       "      <td>3</td>\n",
       "      <td>13.585240</td>\n",
       "      <td>0.127081</td>\n",
       "      <td>19.773667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               n_samples  5.6_n_samples  5.6_avg_Area  5.6_std_Area  \\\n",
       "CL_1                  10           10.0      2.722588      0.436106   \n",
       "CL_2                  10           10.0      3.468092      0.189681   \n",
       "CL_3                  10           10.0      2.980861      0.198321   \n",
       "CL_4                  10           10.0      3.732605      0.211640   \n",
       "CL_whiskey_08          9            NaN           NaN           NaN   \n",
       "CL_ROH_07              9            NaN           NaN           NaN   \n",
       "CL_Lismore             9            NaN           NaN           NaN   \n",
       "CL_Clarlyle            9            NaN           NaN           NaN   \n",
       "CL_Evan                3            NaN           NaN           NaN   \n",
       "\n",
       "               5.6_avg_RT  5.9_n_samples  5.9_avg_Area  5.9_std_Area  \\\n",
       "CL_1               5.6264            NaN           NaN           NaN   \n",
       "CL_2               5.6106            NaN           NaN           NaN   \n",
       "CL_3               5.6106            NaN           NaN           NaN   \n",
       "CL_4               5.6058            NaN           NaN           NaN   \n",
       "CL_whiskey_08         NaN            NaN           NaN           NaN   \n",
       "CL_ROH_07             NaN            NaN           NaN           NaN   \n",
       "CL_Lismore            NaN            1.0       3.84904           0.0   \n",
       "CL_Clarlyle           NaN            NaN           NaN           NaN   \n",
       "CL_Evan               NaN            NaN           NaN           NaN   \n",
       "\n",
       "               5.9_avg_RT  6.4_n_samples  ...  18.7_std_Area  18.7_avg_RT  \\\n",
       "CL_1                  NaN            NaN  ...            NaN          NaN   \n",
       "CL_2                  NaN           10.0  ...            NaN          NaN   \n",
       "CL_3                  NaN            NaN  ...            NaN          NaN   \n",
       "CL_4                  NaN           10.0  ...            NaN          NaN   \n",
       "CL_whiskey_08         NaN            NaN  ...            NaN          NaN   \n",
       "CL_ROH_07             NaN            9.0  ...            0.0       18.785   \n",
       "CL_Lismore          5.937            8.0  ...            NaN          NaN   \n",
       "CL_Clarlyle           NaN            5.0  ...            NaN          NaN   \n",
       "CL_Evan               NaN            3.0  ...            NaN          NaN   \n",
       "\n",
       "               19.5_n_samples  19.5_avg_Area  19.5_std_Area  19.5_avg_RT  \\\n",
       "CL_1                      NaN            NaN            NaN          NaN   \n",
       "CL_2                      NaN            NaN            NaN          NaN   \n",
       "CL_3                      NaN            NaN            NaN          NaN   \n",
       "CL_4                      NaN            NaN            NaN          NaN   \n",
       "CL_whiskey_08             NaN            NaN            NaN          NaN   \n",
       "CL_ROH_07                 9.0      34.402772       0.682500    19.573222   \n",
       "CL_Lismore                9.0      37.887647       0.264937    19.576000   \n",
       "CL_Clarlyle               9.0      11.188203       0.063288    19.583222   \n",
       "CL_Evan                   3.0      42.507867       0.030428    19.561333   \n",
       "\n",
       "               19.7_n_samples  19.7_avg_Area  19.7_std_Area  19.7_avg_RT  \n",
       "CL_1                       10     257.989241       7.098318    19.792000  \n",
       "CL_2                       10     526.351981       8.239846    19.793300  \n",
       "CL_3                       10       4.900047       0.181932    19.817500  \n",
       "CL_4                       10      60.634962       1.624316    19.787000  \n",
       "CL_whiskey_08               9       7.834658       0.126155    19.785111  \n",
       "CL_ROH_07                   9      14.742714       0.234583    19.783667  \n",
       "CL_Lismore                  9      18.654211       0.283424    19.785222  \n",
       "CL_Clarlyle                 9       5.867513       0.161361    19.793111  \n",
       "CL_Evan                     3      13.585240       0.127081    19.773667  \n",
       "\n",
       "[9 rows x 53 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peakdir = os.path.join(top, 'peak-area')\n",
    "\n",
    "df_total = pd.read_csv(os.path.join(peakdir, 'total.csv'))\n",
    "pattern = r'\\d+\\.\\d+'\n",
    "RTs = set()\n",
    "for col in df_total.columns:\n",
    "    match = re.findall(pattern, col)\n",
    "    for m in match:\n",
    "        RTs.add(float(m))\n",
    "\n",
    "RTs = sorted(RTs)\n",
    "print(RTs)\n",
    "\n",
    "columns = []\n",
    "for rt in RTs:\n",
    "    columns.append(f\"{rt}_n_samples\")\n",
    "    columns.append(f\"{rt}_avg_Area\")\n",
    "    columns.append(f\"{rt}_std_Area\")\n",
    "    columns.append(f\"{rt}_avg_RT\")\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df.insert(0, 'n_samples', None)\n",
    "\n",
    "pattern = r'(\\d+\\.\\d+)_(Peak_Area|actual-RT)'\n",
    "\n",
    "for name in names:\n",
    "    base, ext = os.path.splitext(name)\n",
    "    csv = os.path.join(peakdir, f'{base}.csv')\n",
    "    id = os.path.basename(csv)\n",
    "    base, ext = os.path.splitext(id)\n",
    "    id = base\n",
    "\n",
    "    row_insert = {}\n",
    "\n",
    "    df_iter = pd.read_csv(csv)\n",
    "    row_insert['n_samples'] = len(df_iter)\n",
    "\n",
    "    for col in df_iter.columns:\n",
    "        match = re.match(pattern, col)\n",
    "        if match:\n",
    "            rt = float(match.group(1))\n",
    "            col_type = match.group(2)\n",
    "\n",
    "            row_list = df_iter[col].dropna().to_numpy()\n",
    "            if col_type == 'Peak_Area':\n",
    "                row_insert[f\"{rt}_n_samples\"] = len(row_list)\n",
    "                row_insert[f\"{rt}_avg_Area\"] = np.mean(row_list)\n",
    "                row_insert[f\"{rt}_std_Area\"] = np.std(row_list)\n",
    "            if col_type == 'actual-RT':\n",
    "                row_insert[f\"{rt}_avg_RT\"] = np.mean(row_list)\n",
    "    df.loc[id] = row_insert\n",
    "\n",
    "df.to_csv(os.path.join(peakdir, 'peak_area.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03381491",
   "metadata": {},
   "source": [
    "## Seperate Standards and Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d175fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "standards = ['CL_1', 'CL_2', 'CL_3', 'CL_4']\n",
    "samples = ['CL_whiskey_08', 'CL_ROH_07', 'CL_Lismore', 'CL_Clarlyle', 'CL_Evan']\n",
    "\n",
    "df_standard = df.loc[standards]\n",
    "df_samples = df.loc[samples]\n",
    "\n",
    "df_standard = df_standard.dropna(axis=1, how='all')\n",
    "df_samples = df_samples.dropna(axis=1, how='all')\n",
    "\n",
    "df_standard.to_csv(os.path.join(peakdir,'standards.csv'))\n",
    "df_samples.to_csv(os.path.join(peakdir, 'samples.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72faa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
